barplot(counts, main="Attrition Distribution by Job Satisfaction", xlab="Job Satistfaction", col=c("lightgreen","red"), legend = rownames(counts), beside=T)
#KNN Algorithm
n<-sapply(my_dataset,function(x){is.numeric(x)})
check n
check values n
list n
numerics<-my_dataset[,n]
summary(numerics)
normalize <- function(x) { return ((x - min(x)) / (max(x) - min(x))) }
numericsNormal <- normalize(numerics)
View(numericsNormal)
titanicDataKNN<-titanicData[,!n]
my_datasetKNN<-my_dataset[,!n]
View(my_datasetKNN)
titanicDataKNN<-cbind(titanicDataKNN,numericsNormal)
my_datasetKNN<-cbind(my_datasetKNN,numericsNormal)
install.packages("dummies")
library(dummies)
library(dummies)
library(dummies)
View(my_dataset)
#handle even categorical data to represent as 1 and 0
#not taking dependent variable
tkNN<-dummy.data.frame(my_dataset[,-2])
summary(tkNN)
training_KNN<-tkNN[index,]
str(n)
training_KNN<-tkNN[index,]
training_KNN<-tkNN[sample,]
testing_KNN<-tkNN[-sample,]
k2<-round(sqrt(dim(training_KNN)[2])) #sqrt of number of attribute
k1 <- round(sqrt(dim(testing_KNN)[1])) #sqrt of number of instances
k3 <- 7 #a number between 3 and 10
library(class)
knn1 <- knn(train = kNNTraining, test = kNNTesting, cl = train$Attrition, k=k1)
knn1 <- knn(train = training_KNN, test = testing_KNN, cl = train$Attrition, k=k1)
knn2 <- knn(train = training_KNN, test = testing_KNN, cl = train$Attrition, k=k2)
knn3 <- knn(train = training_KNN, test = testing_KNN, cl = train$Attrition, k=k3)
#Accuracy
(knn1Acc <- 1- mean(knn1 != test$Attrition))
(knn2Acc <- 1- mean(knn2 != test$Attrition))
(knn3Acc <- 1- mean(knn3 != test$Attrition))
library(C50)
cFiftyModel<-C50::C5.0(Attrition~.,data = train,trails=10)
class(cFiftyModel)
summary(cFiftyModel)
#plot(cFiftyModel)
cFiftyModelPrediction<-predict(cFiftyModel,newdata=testing[,-dependentColumnNumber])
#plot(cFiftyModel)
cFiftyModelPrediction<-predict(cFiftyModel,newdata=test[,-dependentColumnNumber])
#plot(cFiftyModel)
cFiftyModelPrediction<-predict(cFiftyModel,newdata=test[,-2])
(cFiftyModelAccuracy <- mean(cFiftyModelPrediction == testing$Survived))
#plot(cFiftyModel)
cFiftyModelPrediction<-predict(cFiftyModel,newdata=test[,-2])
cFiftyModel<-C50::C5.0(Attrition~.,data = train,trails=10)
cFiftyModel<-C50::C5.0(train$Attrition~.,data = train,trails=10)
cFiftyModel<-C50::C5.0(train$Attrition~.,data = train,trails=10)
NBEmployee<-my_dataset
str(NBEmployee)
table(NBEmployee$SibSp)
table(NBEmployee$Parch)
#convert SibSp,Parch,Age and Fare to categorical Fsize is removed as FsizeD is derived from it.
NBEmployee$SibSp<-as.factor(NBEmployee$SibSp)
table(NBEmployee$SibSp)
str(NBEmployee)
class(cFiftyModel)
library(C50)
cFiftyModel<-C50::C5.0(train$Attrition~.,data = train,trails=10)
cFiftyModel<-C50::C5.0(train$Attrition~.,data = train,trails=10)
View(my_dataset)
nearZeroVar(my_dataset)
nearZeroVar(my_dataset,saveMetrics = TRUE)
nearZeroVar(train,saveMetrics = TRUE)
str(train)
#let's deal with these other factors: F1
my_dataset$Education <- factor(my_dataset$Education, levels = c(1,2,3,4,5), labels=c("BC", "C", "UG", "MSc", "PhD"))
my_dataset$EnvironmentSatisfaction <- factor(my_dataset$EnvironmentSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$JobInvolvement <- factor(my_dataset$JobInvolvement, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$JobLevel <- as.factor(my_dataset$JobLevel) #insufficient information to do more
my_dataset$JobSatisfaction <- factor(my_dataset$JobSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating, levels = c(1:4), labels=c("Low", "Good", "Excellent", "Outstanding"))
my_dataset$RelationshipSatisfaction <- factor(my_dataset$RelationshipSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$StockOptionLevel <- as.factor(my_dataset$StockOptionLevel) #don't have more information
my_dataset$WorkLifeBalance <- factor(my_dataset$WorkLifeBalance, levels = c(1:4), labels=c("Bad", "Good", "Better", "Best"))
#Now to fix factors where i don't have all levels
summary(my_dataset)
#I'm missing levels 0 and 1 in PerformanceRating, but the rest of my categoricals seem fine
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating)
table(my_dataset$JobRole)
#a couple of the variables are pointless, so let's get rid of them
my_dataset <- my_dataset[,-9] #EmployeeCount -- always 1
#a couple of the variables are pointless, so let's get rid of them
my_dataset <- my_dataset[,-9] #EmployeeCount -- always 1
#mising values:
sapply(my_dataset,function(x) sum(is.na(x)))
hrdata <- read.csv("Sampleadm.csv", stringsAsFactors = T)
set.seed(1337)
#Selecting a random number in a uniform distribution
my_dataset <- hrdata[order(runif(600)), ]
str(my_dataset)
#Lets remove id we dont need that
my_dataset <- my_dataset[-10]
col1 <- round(runif(1)*32)+2 #the +2 protects the Age and Attrition Variables
col2 <- round(runif(1)*31)+2
cols <- names(my_dataset)
print(paste("I lost: ", cols[col1], ",", cols[col2], ",", cols[col3]))
my_dataset <- my_dataset[-col1]
my_dataset <- my_dataset[-col2]
my_dataset <- my_dataset[-col3]
write.csv(file="Emp.csv", my_dataset, row.names = F)
my_dataset <- read.csv("Emp.csv", stringsAsFactors = T)
str(my_dataset)
#let's deal with these other factors: F1
my_dataset$Education <- factor(my_dataset$Education, levels = c(1,2,3,4,5), labels=c("BC", "C", "UG", "MSc", "PhD"))
my_dataset$EnvironmentSatisfaction <- factor(my_dataset$EnvironmentSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$JobInvolvement <- factor(my_dataset$JobInvolvement, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$JobLevel <- as.factor(my_dataset$JobLevel) #insufficient information to do more
my_dataset$JobSatisfaction <- factor(my_dataset$JobSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating, levels = c(1:4), labels=c("Low", "Good", "Excellent", "Outstanding"))
my_dataset$RelationshipSatisfaction <- factor(my_dataset$RelationshipSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$StockOptionLevel <- as.factor(my_dataset$StockOptionLevel) #don't have more information
my_dataset$WorkLifeBalance <- factor(my_dataset$WorkLifeBalance, levels = c(1:4), labels=c("Bad", "Good", "Better", "Best"))
#Now to fix factors where i don't have all levels
summary(my_dataset)
#I'm missing levels 0 and 1 in PerformanceRating, but the rest of my categoricals seem fine
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating)
table(my_dataset$JobRole)
#a couple of the variables are pointless, so let's get rid of them
my_dataset <- my_dataset[,-9] #EmployeeCount -- always 1
#mising values:
sapply(my_dataset,function(x) sum(is.na(x)))
#outliers
boxplot(subset(my_dataset, select=c(1,6,18,20,21:24))) #nothing too obvious here
boxplot(subset(my_dataset, select=c(26:29))) #May be something in years at company
boxplot(my_dataset$DailyRate)
boxplot(my_dataset$HourlyRate)
boxplot(my_dataset$MonthlyIncome) #possibly some here too
summary(my_dataset$MonthlyRate)
#i prefer the attrition factor as 0/1 for convenience
my_dataset$Attrition <- factor(my_dataset$Attrition, labels=c(0,1), levels=c("No", "Yes"))
#checking the class balance
table(my_dataset$Attrition) #its imbalanced
#train and testing the dataset
library(caret)
sample <- createDataPartition(my_dataset$Attrition, p = .75, list = FALSE)
train <- my_dataset[sample, ]
test <- my_dataset[-sample, ]
library(C50)
nearZeroVar(train,saveMetrics = TRUE)
str(train)
cFiftyModel<-C50::C5.0(train$Attrition~.,data = train,trails=10)
my_dataset <- my_dataset[,-19]
nearZeroVar(train,saveMetrics = TRUE)
my_dataset <- my_dataset[-19]
nearZeroVar(train,saveMetrics = TRUE)
my_dataset$Over18 <- NULL
nearZeroVar(train,saveMetrics = TRUE)
my_dataset <- my_dataset[-19]
hrdata <- read.csv("Sampleadm.csv", stringsAsFactors = T)
set.seed(1337)
#Selecting a random number in a uniform distribution
my_dataset <- hrdata[order(runif(600)), ]
str(my_dataset)
#Lets remove id we dont need that
my_dataset <- my_dataset[-10]
col1 <- round(runif(1)*32)+2 #the +2 protects the Age and Attrition Variables
col2 <- round(runif(1)*31)+2
col3 <- round(runif(1)*30)+2
cols <- names(my_dataset)
print(paste("I lost: ", cols[col1], ",", cols[col2], ",", cols[col3]))
my_dataset <- my_dataset[-col1]
my_dataset <- my_dataset[-col2]
my_dataset <- my_dataset[-col3]
str(my_dataset)
#let's deal with these other factors: F1
my_dataset$Education <- factor(my_dataset$Education, levels = c(1,2,3,4,5), labels=c("BC", "C", "UG", "MSc", "PhD"))
my_dataset$EnvironmentSatisfaction <- factor(my_dataset$EnvironmentSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$JobInvolvement <- factor(my_dataset$JobInvolvement, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$JobLevel <- as.factor(my_dataset$JobLevel) #insufficient information to do more
my_dataset$JobSatisfaction <- factor(my_dataset$JobSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating, levels = c(1:4), labels=c("Low", "Good", "Excellent", "Outstanding"))
my_dataset$RelationshipSatisfaction <- factor(my_dataset$RelationshipSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$StockOptionLevel <- as.factor(my_dataset$StockOptionLevel) #don't have more information
my_dataset$WorkLifeBalance <- factor(my_dataset$WorkLifeBalance, levels = c(1:4), labels=c("Bad", "Good", "Better", "Best"))
#Now to fix factors where i don't have all levels
summary(my_dataset)
#I'm missing levels 0 and 1 in PerformanceRating, but the rest of my categoricals seem fine
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating)
table(my_dataset$JobRole)
#a couple of the variables are pointless, so let's get rid of them
my_dataset <- my_dataset[,-9] #EmployeeCount -- always 1
my_dataset <- my_dataset[, -19] #over 18 -- always Y; strictly speaking we should confirm this, but under time pressure it's safe to assume this varibale isn't needed
#mising values:
sapply(my_dataset,function(x) sum(is.na(x)))
#i prefer the attrition factor as 0/1 for convenience
my_dataset$Attrition <- factor(my_dataset$Attrition, labels=c(0,1), levels=c("No", "Yes"))
#train and testing the dataset
library(caret)
sample <- createDataPartition(my_dataset$Attrition, p = .75, list = FALSE)
train <- my_dataset[sample, ]
test <- my_dataset[-sample, ]
#Making some own assumptions:
#Everyonestays
str(test$Attrition) # remain (Y) is 0
b1 <- rep(0, dim(test)[1])
(accuracyB1 <- 1 - mean(b1 != test$Attrition))
#those who have higher satisfaction are less likely to leave
str(my_dataset$JobSatisfaction)
b2 <- rep(0, dim(test)[1])
b2[test$JobSatisfaction == 'Low'] <- 1
b2[test$JobSatisfaction == 'Medium'] <- 1
(accuracyB2 <- 1 - mean(b2 != test$Attrition))
#work out what columns we have:
str(subset(my_dataset, select=c(1,4,6,10,16:19,23:24,27,26:29)))
#now correlation testew
cor(subset(my_dataset, select=c(1,4,6,10,16:19,23:24,27,26:29)))
library(C50)
nearZeroVar(train,saveMetrics = TRUE)
str(train)
my_dataset <- my_dataset[-19]
nearZeroVar(train,saveMetrics = TRUE)
cFiftyModel<-C50::C5.0(train$Attrition~.,data = train,trails=10)
hrdata <- read.csv("Sampleadm.csv", stringsAsFactors = T)
set.seed(1337)
#Selecting a random number in a uniform distribution
my_dataset <- hrdata[order(runif(600)), ]
str(my_dataset)
#Lets remove id we dont need that
my_dataset <- my_dataset[-10]
col1 <- round(runif(1)*32)+2 #the +2 protects the Age and Attrition Variables
col2 <- round(runif(1)*31)+2
col3 <- round(runif(1)*30)+2
cols <- names(my_dataset)
print(paste("I lost: ", cols[col1], ",", cols[col2], ",", cols[col3]))
my_dataset <- my_dataset[-col1]
my_dataset <- my_dataset[-col2]
my_dataset <- my_dataset[-col3]
write.csv(file="Emp.csv", my_dataset, row.names = F)
my_dataset <- read.csv("Emp.csv", stringsAsFactors = T)
str(my_dataset)
#let's deal with these other factors: F1
my_dataset$Education <- factor(my_dataset$Education, levels = c(1,2,3,4,5), labels=c("BC", "C", "UG", "MSc", "PhD"))
my_dataset$EnvironmentSatisfaction <- factor(my_dataset$EnvironmentSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$JobInvolvement <- factor(my_dataset$JobInvolvement, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$JobLevel <- as.factor(my_dataset$JobLevel) #insufficient information to do more
my_dataset$JobSatisfaction <- factor(my_dataset$JobSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating, levels = c(1:4), labels=c("Low", "Good", "Excellent", "Outstanding"))
my_dataset$RelationshipSatisfaction <- factor(my_dataset$RelationshipSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$StockOptionLevel <- as.factor(my_dataset$StockOptionLevel) #don't have more information
my_dataset$WorkLifeBalance <- factor(my_dataset$WorkLifeBalance, levels = c(1:4), labels=c("Bad", "Good", "Better", "Best"))
str(my_dataset)
table(my_dataset$BusinessTravel)
hrdata <- read.csv("Sampleadm.csv", stringsAsFactors = T)
set.seed(1337)
#Selecting a random number in a uniform distribution
my_dataset <- hrdata[order(runif(600)), ]
str(my_dataset)
#Lets remove id we dont need that
my_dataset <- my_dataset[-10]
col1 <- round(runif(1)*32)+2 #the +2 protects the Age and Attrition Variables
col2 <- round(runif(1)*31)+2
col3 <- round(runif(1)*30)+2
cols <- names(my_dataset)
print(paste("I lost: ", cols[col1], ",", cols[col2], ",", cols[col3]))
my_dataset <- my_dataset[-col1]
my_dataset <- my_dataset[-col2]
my_dataset <- my_dataset[-col3]
write.csv(file="Emp.csv", my_dataset, row.names = F)
my_dataset <- read.csv("Emp.csv", header = T)
str(my_dataset)
#let's deal with these other factors: F1
my_dataset$Education <- factor(my_dataset$Education, levels = c(1,2,3,4,5), labels=c("BC", "C", "UG", "MSc", "PhD"))
my_dataset$EnvironmentSatisfaction <- factor(my_dataset$EnvironmentSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$JobInvolvement <- factor(my_dataset$JobInvolvement, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$JobLevel <- as.factor(my_dataset$JobLevel) #insufficient information to do more
my_dataset$JobSatisfaction <- factor(my_dataset$JobSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating, levels = c(1:4), labels=c("Low", "Good", "Excellent", "Outstanding"))
my_dataset$RelationshipSatisfaction <- factor(my_dataset$RelationshipSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$StockOptionLevel <- as.factor(my_dataset$StockOptionLevel) #don't have more information
my_dataset$WorkLifeBalance <- factor(my_dataset$WorkLifeBalance, levels = c(1:4), labels=c("Bad", "Good", "Better", "Best"))
str(my_dataset)
library(caret)
sample <- createDataPartition(my_dataset$Attrition, p = .75, list = FALSE)
train <- my_dataset[sample, ]
test <- my_dataset[-sample, ]
library(C50)
nearZeroVar(train,saveMetrics = TRUE)
str(train)
my_dataset <- my_dataset[-9]
my_dataset <- my_dataset[, -19
]
library(caret)
sample <- createDataPartition(my_dataset$Attrition, p = .75, list = FALSE)
train <- my_dataset[sample, ]
test <- my_dataset[-sample, ]
library(C50)
nearZeroVar(train,saveMetrics = TRUE)
cFiftyModel<-C50::C5.0(train$Attrition~.,data = train,trails=10)
cFiftyModel<-C5.0(train$Attrition~.,data = train,trails=10)
str(train)
cFiftyModel<-C5.0(train$Attrition~.,data = train[1:4],trails=10)
cFiftyModel<-C5.0(train$Attrition~.,data = train[1:10],trails=10)
cFiftyModel<-C5.0(train$Attrition~.,data = train[11:29],trails=10)
cFiftyModel<-C5.0(train$Attrition~.,data = train[1:9],trails=10)
cFiftyModel<-C5.0(train$Attrition~.,data = train[1:7],trails=10)
cFiftyModel<-C5.0(train$Attrition~.,data = train[1:8],trails=10)
table(train$EnvironmentSatisfaction)
hrdata <- read.csv("Sampleadm.csv", stringsAsFactors = T)
set.seed(1337)
#Selecting a random number in a uniform distribution
my_dataset <- hrdata[order(runif(600)), ]
str(my_dataset)
#Lets remove id we dont need that
my_dataset <- my_dataset[-10]
col1 <- round(runif(1)*32)+2 #the +2 protects the Age and Attrition Variables
col2 <- round(runif(1)*31)+2
col3 <- round(runif(1)*30)+2
cols <- names(my_dataset)
print(paste("I lost: ", cols[col1], ",", cols[col2], ",", cols[col3]))
my_dataset <- my_dataset[-col1]
my_dataset <- my_dataset[-col2]
my_dataset <- my_dataset[-col3]
write.csv(file="Emp.csv", my_dataset, row.names = F)
my_dataset <- read.csv("Emp.csv", header = T)
str(my_dataset)
#let's deal with these other factors: F1
my_dataset$Education <- factor(my_dataset$Education, levels = c(1,2,3,4,5), labels=c("BC", "C", "UG", "MSc", "PhD"))
my_dataset$EnvironmentSatisfaction <- factor(my_dataset$EnvironmentSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", " VHigh"))
my_dataset$JobInvolvement <- factor(my_dataset$JobInvolvement, levels = c(1:4), labels=c("Low", "Medium", "High", "vHigh"))
my_dataset$JobLevel <- as.factor(my_dataset$JobLevel) #insufficient information to do more
my_dataset$JobSatisfaction <- factor(my_dataset$JobSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "vHigh"))
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating, levels = c(1:4), labels=c("Low", "Good", "Excellent", "Outstanding"))
my_dataset$RelationshipSatisfaction <- factor(my_dataset$RelationshipSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "vHigh"))
my_dataset$StockOptionLevel <- as.factor(my_dataset$StockOptionLevel) #don't have more information
my_dataset$WorkLifeBalance <- factor(my_dataset$WorkLifeBalance, levels = c(1:4), labels=c("Bad", "Good", "Better", "Best"))
my_dataset <- my_dataset[,-9] #EmployeeCount -- always 1
my_dataset <- my_dataset[, -19]
#train and testing the dataset
library(caret)
sample <- createDataPartition(my_dataset$Attrition, p = .75, list = FALSE)
train <- my_dataset[sample, ]
test <- my_dataset[-sample, ]
library(C50)
nearZeroVar(train,saveMetrics = TRUE)
(train)[9] <-
cFiftyModel<-C5.0(train$Attrition~.,data = train[1:8],trails=10)
str(train)
cFiftyModel<-C5.0(train$Attrition~.,data = train,trails=10)
class(cFiftyModel)
summary(cFiftyModel)
#plot(cFiftyModel)
cFiftyModelPrediction<-predict(cFiftyModel,newdata=test[,-2])
(cFiftyModelAccuracy <- mean(cFiftyModelPrediction == test$Attrition))
library(C50)
nearZeroVar(train,saveMetrics = TRUE) #To chweck for the unwanted rows
cFiftyModel<-C5.0(train$Attrition~.,data = train[1:8],trails=10)
cFiftyModel<-C5.0(train$Attrition~.,data = train,trails=10)
class(cFiftyModel)
summary(cFiftyModel)
#plot(cFiftyModel)
cFiftyModelPrediction<-predict(cFiftyModel,newdata=test[,-2])
(cFiftyModelAccuracy <- mean(cFiftyModelPrediction == test$Attrition))
nb <- naiveBayes(Attrition ~., data=train)
nb <- e1071::naiveBayes(Attrition ~., data=train)
nbP <- predict(nb, newdata=test[,-2], type = "class")
(nbAcc <- 1- mean(nbP != test$Attrition))
NBData<-my_dataset
str(NBData)
table(NBData$Age)
str(NBData)
#table(NBData$Age)
#table(NBTitanicData$Parch)
#convert SibSp,Parch,Age and Fare to categorical Fsize is removed as FsizeD is derived from it.
NBData$Age<-as.factor(NBData$Age)
setwd("C:/Nithi/Github/MyProgramming")
my_dataset <- read.csv("Emp.csv", header = T)
trainingNB<-NBTitanicData[index,]
NBData<-my_dataset
str(NBData)
#table(NBData$Age)
#table(NBTitanicData$Parch)
#convert SibSp,Parch,Age and Fare to categorical Fsize is removed as FsizeD is derived from it.
NBData$Age<-as.factor(NBData$Age)
str(NBData)
table(NBData$DailyRate)
#table(NBTitanicData$Parch)
#convert SibSp,Parch,Age and Fare to categorical Fsize is removed as FsizeD is derived from it.
NBData$DailyRate<-as.factor(NBData$DailyRate)
#checking the class balance
prop.table(my_dataset$Attrition) #its imbalanced
table(NBData$Age)
View(NBData)
str(NBData) able(NBData$Age)
str(NBData)
View(NBData)
View(NBData)
#checking the class balance
table(my_dataset$Attrition) #its imbalanced
prop.table(table(my_dateset$Attrition))
prop.table(table(my_dataset$Attrition))
sample <- createDataPartition(my_dataset$Attrition, p = .75, list = FALSE)
train <- my_dataset[sample, ]
prop.table(table(train$Attrition))
str(NBData)
table(NBData$i..Age)
table(NBData$ï..Age)
#table(NBData$ï..Age)
NBData$StockOptionLevel<-as.factor(NBData$StockOptionLevel)
str(NBData$StockOptionLevel)
NBData$WorkLifeBalance<-as.factor(NBData$WorkLifeBalance)
NBData$EmployeeCount<-NULL
str(NBData)
#
hist(NBData$ï..Age, breaks = 30)
#
hist(NBData$ï..Age, breaks = 50)
#
hist(NBData$ï..Age, breaks = 30)
#
hist(NBData$ï..Age, breaks = 30)
library(OneR)
install.packages("OneR")
library(OneR)
library(OneR)
res1 <- bin(NBData$ï..Age, nbins = 10, method = "content", na.omit = TRUE)
NBData<-cbind(NBData,BinnedAge=res1)
table(NBData$BinnedAge,NBData$Over18)
#HourlyRate
hist(NBData$HourlyRate) #seems normally distributed
res1 <- bin(NBData$HourlyRate, nbins = 15, method = "content", na.omit = TRUE)
res1 <- bin(NBData$ï..Age, nbins = 10, method = "content", na.omit = TRUE)
NBData<-cbind(NBData,BinnedAge=res1)
table(NBData$BinnedAge,NBData$Over18)
#HourlyRate
hist(NBData$HourlyRate) #seems normally distributed
res2 <- bin(NBData$HourlyRate, nbins = 15, method = "content", na.omit = TRUE)
NBData<-cbind(NBData,BinnedHour=res2)
table(NBData$BinnedHour,NBData$Department)
trainingNB<-NBData[sample,]
testingNB<-NBTitanicData[-sample,]
testingNB<-NBData[-sample,]
nb <- e1071::naiveBayes(trainingNB, train$Attrition)
nbP <- predict(nb, newdata=testingNB[,-2], type = "class")
(nbAcc <- 1- mean(nbP != test$Attrition))
library(rpart)
library(rpart.plot)
install.packages("rpart.plot")
library(rpart.plot)
library(RColorBrewer)
regressionTree <- rpart::rpart(Attrition ~ ., data=train, method="class")
library(rattle)
install.packages("rattle")
library(rattle)
fancyRpartPlot(regressionTree)
fancyRpartPlot(regressionTree)
rpartPrediction <- predict(regressionTree, test, type = "class")
my_dataset <- read.csv("Emp.csv", header = T)
my_dataset <- read.csv("Emp.csv", header = T)
str(my_dataset)
#let's deal with these other factors: F1
my_dataset$Education <- factor(my_dataset$Education, levels = c(1,2,3,4,5), labels=c("BC", "C", "UG", "MSc", "PhD"))
my_dataset$EnvironmentSatisfaction <- factor(my_dataset$EnvironmentSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", " VHigh"))
my_dataset$JobInvolvement <- factor(my_dataset$JobInvolvement, levels = c(1:4), labels=c("Low", "Medium", "High", "vHigh"))
my_dataset$JobLevel <- as.factor(my_dataset$JobLevel) #insufficient information to do more
my_dataset$JobSatisfaction <- factor(my_dataset$JobSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "vHigh"))
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating, levels = c(1:4), labels=c("Low", "Good", "Excellent", "Outstanding"))
my_dataset$RelationshipSatisfaction <- factor(my_dataset$RelationshipSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "vHigh"))
my_dataset$StockOptionLevel <- as.factor(my_dataset$StockOptionLevel) #don't have more information
my_dataset$WorkLifeBalance <- factor(my_dataset$WorkLifeBalance, levels = c(1:4), labels=c("Bad", "Good", "Better", "Best"))
table(my_dataset$BusinessTravel)
#Now to fix factors where i don't have all levels
summary(my_dataset)
#I'm missing levels 0 and 1 in PerformanceRating, but the rest of my categoricals seem fine
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating)
#however, as JobRole displays other, let's check more precisely
table(my_dataset$JobRole)
#because i already encoded them above, all i have to do is reinvoke the factor command and my empty levels disappear
#a couple of the variables are pointless, so let's get rid of them
my_dataset <- my_dataset[,-9] #EmployeeCount -- always 1
my_dataset <- my_dataset[, -19] #over 18 -- always Y; strictly speaking we should confirm this, but under time pressure it's safe to assume this varibale isn't needed
#mising values:
sapply(my_dataset,function(x) sum(is.na(x)))
#outliers
boxplot(subset(my_dataset, select=c(1,6,18,20,21:24))) #nothing too obvious here
boxplot(subset(my_dataset, select=c(26:29))) #May be something in years at company
boxplot(my_dataset$DailyRate)
boxplot(my_dataset$HourlyRate)
boxplot(my_dataset$MonthlyIncome) #possibly some here too
summary(my_dataset$MonthlyRate)
#i prefer the attrition factor as 0/1 for convenience
my_dataset$Attrition <- factor(my_dataset$Attrition, labels=c(0,1), levels=c("No", "Yes"))
#checking the class balance
table(my_dataset$Attrition) #its imbalanced
prop.table(table(my_dataset$Attrition))
#train and testing the dataset
library(caret)
sample <- createDataPartition(my_dataset$Attrition, p = .75, list = FALSE)
train <- my_dataset[sample, ]
prop.table(table(train$Attrition))
test <- my_dataset[-sample, ]
regressionTree <- rpart::rpart(Attrition ~ ., data=train, method="class")
install.packages("rattle")
library(rattle)
fancyRpartPlot(regressionTree)
rpartPrediction <- predict(regressionTree, test, type = "class")
(CARTModelAccuracy <- mean(rpartPrediction == test$Attrition))
#Pruning in CART
# what does manual pruning help in ? how is it working?
treeToPrune <- rpart(Attrition ~ ., data=train, method="class", control=rpart.control(minsplit=,cp=0))
# Ask aastha
prunedTree <- prp(treeToPrune,snip=TRUE)$obj
