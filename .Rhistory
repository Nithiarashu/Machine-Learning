1
head(tweets.df$text)
library(devtools)
library(twitteR)
#library(tm)
#library(wordcloud)
#library(RColorBrewer)
#library(SnowballC)
api_key = "yW7uoG9wm8O88zUwrmoENQlAO"
api_secret = "CQbsWoEj6V01GRFMyXE6P5ZFbjqoninCBGSl5eBYnLJeUpjRuK"
access_token = "4477109293-HgaeW875Jv5V7XNal8rZZvXpAc2iYY3Ba7GgM1y"
access_token_secret = "pHIa1hlDVPKx0pGsU7HbrmRkQA18KAeOB4VCHFc3qeR8U"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
keywords = c("#Terrorism")
tweets = searchTwitter(keywords, n=2000, lang='en')
tweets.df = twListToDF(tweets)
dim(tweets.df)
head(tweets.df$text)
dim(tweets.df)
install.packages("stringr")
library(devtools)
library(twitteR)
library(stringr)
#library(tm)
#library(wordcloud)
#library(RColorBrewer)
#library(SnowballC)
api_key = "yW7uoG9wm8O88zUwrmoENQlAO"
api_secret = "CQbsWoEj6V01GRFMyXE6P5ZFbjqoninCBGSl5eBYnLJeUpjRuK"
access_token = "4477109293-HgaeW875Jv5V7XNal8rZZvXpAc2iYY3Ba7GgM1y"
access_token_secret = "pHIa1hlDVPKx0pGsU7HbrmRkQA18KAeOB4VCHFc3qeR8U"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
keywords = c("#Terrorism")
tweets = searchTwitter(keywords, n=2000, lang='en')
tweets.df = twListToDF(tweets)
dim(tweets.df)
tweets.df$text
iconv(tweets.df$text, "ISO_8859-2", "UTF-8")
iconv(tweets.df$text, "LATIN2", "UTF-8")
install.packages(c("rjson", "bit64", "httr", "doBy", "XML", "base64enc"))
install.packages(c("rjson", "bit64", "httr", "doBy", "XML", "base64enc"))
install_github("geoffjentry/twitteR")
install_github('R-package','quandl')
install.packages("Quandl")
#install.packages(c("rjson", "bit64", "httr", "doBy", "XML", "base64enc"))
library(devtools)
#install_github("geoffjentry/twitteR")
#install_github('R-package','quandl')
library(plyr)
library(httr)
library(doBy)
library(Quandl)
library(twitteR)
api_key = "yW7uoG9wm8O88zUwrmoENQlAO"
api_secret = "CQbsWoEj6V01GRFMyXE6P5ZFbjqoninCBGSl5eBYnLJeUpjRuK"
access_token = "4477109293-HgaeW875Jv5V7XNal8rZZvXpAc2iYY3Ba7GgM1y"
access_token_secret = "pHIa1hlDVPKx0pGsU7HbrmRkQA18KAeOB4VCHFc3qeR8U"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
hu.liu.pos = scan('positive-words.txt', what='character', comment.char=';')
hu.liu.neg = scan('negative-words.txt', what='character', comment.char=';')
#now we can add some domain-specific terminolgy
pos.words = c(hu.liu.pos, 'fair','growth','good','economy')
neg.words = c(hu.liu.neg, 'terror', 'attack', 'bomb', 'suicide','unacceptable','fall')
#our first function
score.sentence <- function(sentence, pos.words, neg.words) {
#here some basic cleaning
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl::]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
sentence = tolower(sentence)
#basic data structure construction
word.list = str_split(sentence, '\\s+')
words = unlist(word.list)
#here we count the number of words that are positive and negative
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
#throw away those that didn't match
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
#compute the sentiment score
score = sum(pos.matches) - sum(neg.matches)
return(score)
}
#our second function that takes an array of sentences and sentiment analyses them
score.sentiment <- function(sentences, pos.words, neg.words) {
require(plyr)
require(stringr)
#here any sentence/tweet that causes an error is given a sentiment score of 0 (neutral)
scores = laply(sentences, function(sentence, pos.words, neg.words) {
tryCatch(score.sentence(sentence, pos.words, neg.words ), error=function(e) 0)
}, pos.words, neg.words)
#now we construct a data frame
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
#our third function, that communicates with twitter and then scores each of the tweets returned
collect.and.score <- function (handle, countryName, pos.words, neg.words) {
tweets = searchTwitter(handle, n=500)
text = laply(tweets, function(t) t$getText())
score = score.sentiment(text, pos.words, neg.words)
score$countryName = countryName
#  score$code = code
return (score)
}
#here we invoke the function above for each of our airlines
DomRep.scores = collect.and.score("#DominicanRepublic","Dominican Republic", pos.words, neg.words)
Mexico.scores = collect.and.score("#Mexico","Mexico", pos.words, neg.words)
#install.packages(c("rjson", "bit64", "httr", "doBy", "XML", "base64enc"))
library(devtools)
#install_github("geoffjentry/twitteR")
#install_github('R-package','quandl')
library(plyr)
library(httr)
library(doBy)
library(Quandl)
library(twitteR)
api_key = "yW7uoG9wm8O88zUwrmoENQlAO"
api_secret = "CQbsWoEj6V01GRFMyXE6P5ZFbjqoninCBGSl5eBYnLJeUpjRuK"
access_token = "4477109293-HgaeW875Jv5V7XNal8rZZvXpAc2iYY3Ba7GgM1y"
access_token_secret = "pHIa1hlDVPKx0pGsU7HbrmRkQA18KAeOB4VCHFc3qeR8U"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
hu.liu.pos = scan('positive-words.txt', what='character', comment.char=';')
hu.liu.neg = scan('negative-words.txt', what='character', comment.char=';')
#now we can add some domain-specific terminolgy
pos.words = c(hu.liu.pos, 'fair','growth','good','economy')
neg.words = c(hu.liu.neg, 'terror', 'attack', 'bomb', 'suicide','unacceptable','fall')
#our first function
score.sentence <- function(sentence, pos.words, neg.words) {
#here some basic cleaning
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl::]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
sentence = tolower(sentence)
#basic data structure construction
word.list = str_split(sentence, '\\s+')
words = unlist(word.list)
#here we count the number of words that are positive and negative
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
#throw away those that didn't match
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
#compute the sentiment score
score = sum(pos.matches) - sum(neg.matches)
return(score)
}
#our second function that takes an array of sentences and sentiment analyses them
score.sentiment <- function(sentences, pos.words, neg.words) {
require(plyr)
require(stringr)
#here any sentence/tweet that causes an error is given a sentiment score of 0 (neutral)
scores = laply(sentences, function(sentence, pos.words, neg.words) {
tryCatch(score.sentence(sentence, pos.words, neg.words ), error=function(e) 0)
}, pos.words, neg.words)
#now we construct a data frame
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
#our third function, that communicates with twitter and then scores each of the tweets returned
collect.and.score <- function (handle, countryName, pos.words, neg.words) {
tweets = searchTwitter(handle, n=500)
text = laply(tweets, function(t) t$getText())
score = score.sentiment(text, pos.words, neg.words)
score$countryName = countryName
#  score$code = code
return (score)
}
#here we invoke the function above for each of our airlines
DomRep.scores = collect.and.score("#DominicanRepublic","Dominican Republic", pos.words, neg.words)
Mexico.scores = collect.and.score("#Mexico","Mexico", pos.words, neg.words)
#install.packages(c("rjson", "bit64", "httr", "doBy", "XML", "base64enc"))
library(devtools)
#install_github("geoffjentry/twitteR")
#install_github('R-package','quandl')
library(plyr)
library(httr)
library(doBy)
library(Quandl)
library(twitteR)
api_key = "PA1FaZycV1Op5FGJdKIPE1coa"
api_secret = "jiUQYtBD9OtvVVygDCXDGn4xI8pBj3xdLKRyOz4wOEIoFbsgE4"
access_token = "2261152723-Va9KBatH7hU0UmlaTQSudQdBNl1omIfnU0orb8J"
access_token_secret = "vadLvMXSxgdRCSUIGKFcg3dJgJjwn60V4BrlnEzRA3G30"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
hu.liu.pos = scan('positive-words.txt', what='character', comment.char=';')
hu.liu.neg = scan('negative-words.txt', what='character', comment.char=';')
#now we can add some domain-specific terminolgy
pos.words = c(hu.liu.pos, 'Indianstartup','good','Startupbusiness')
neg.words = c(hu.liu.neg, 'loss', 'notgood', 'badidea','Startuploss')
#our first function
score.sentence <- function(sentence, pos.words, neg.words) {
#here some basic cleaning
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl::]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
sentence = tolower(sentence)
#basic data structure construction
word.list = str_split(sentence, '\\s+')
words = unlist(word.list)
#here we count the number of words that are positive and negative
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
#throw away those that didn't match
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
#compute the sentiment score
score = sum(pos.matches) - sum(neg.matches)
return(score)
}
#our second function that takes an array of sentences and sentiment analyses them
score.sentiment <- function(sentences, pos.words, neg.words) {
require(plyr)
require(stringr)
#here any sentence/tweet that causes an error is given a sentiment score of 0 (neutral)
scores = laply(sentences, function(sentence, pos.words, neg.words) {
tryCatch(score.sentence(sentence, pos.words, neg.words ), error=function(e) 0)
}, pos.words, neg.words)
#now we construct a data frame
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
#our third function, that communicates with twitter and then scores each of the tweets returned
collect.and.score <- function (handle, companyName, pos.words, neg.words) {
tweets = searchTwitter(handle, n=500)
text = laply(tweets, function(t) t$getText())
score = score.sentiment(text, pos.words, neg.words)
score$companyName = companyName
#  score$code = code
return (score)
}
#here we invoke the function above for each of our airlines
CredR.score = collect.and.score("#CredR","CredR", pos.words, neg.words)
quikr.score = collect.and.score("#quikr","Quikr", pos.words, neg.words)
Naaptol.score = collect.and.score("#Naaptol","Maaptol", pos.words, neg.words)
Pepperfry.score = collect.and.score("#Pepperfry","Pepperfry", pos.words, neg.words)
Faasos.score = collect.and.score("#flipkart","Faasos", pos.words, neg.words)
#We can view any of these data frames using the View function, e.g.: View(delta.scores)
View(delta.scores)
#we combine all data frames into 1
all.scores = rbind(CredR.score,quikr.score,Naaptol.score,Pepperfry.score,Faasos.score)
all.scores
#skim only the most positive or negative tweets to throw away noise near 0
all.scores$very.pos = as.numeric( all.scores$score >= 2)
all.scores$very.neg = as.numeric( all.scores$score <= -2)
#now we construct the twitter data frame and simultaneously compute the pos/neg sentiment scores for each airline
twitter.df = ddply(all.scores, c('code', 'company_name'), summarise, pos.count = sum (very.pos), neg.count = sum(very.neg))
#and here the general sentiment
twitter.df$all.count = twitter.df$pos.count + twitter.df$neg.count
#now in order to be able to compare data sets we normalise the sentiment score to be a percentage
twitter.df$score = round (100 * twitter.df$pos.count / twitter.df$all.count)
#and to help understand our data, order by our now normalised score
orderBy(~-score, twitter.df)
#now let's get data source no. 2, for this we need some extra packages to help us parse HTML
write.csv(twitter.df, file="SentimentalStartup.csv")
#We can view any of these data frames using the View function, e.g.: View(delta.scores)
View(delta.scores)
#we combine all data frames into 1
all.scores = rbind(CredR.score,quikr.score,Naaptol.score,Pepperfry.score,Faasos.score)
all.scores
#skim only the most positive or negative tweets to throw away noise near 0
all.scores$very.pos = as.numeric( all.scores$score >= 2)
all.scores$very.neg = as.numeric( all.scores$score <= -2)
#now we construct the twitter data frame and simultaneously compute the pos/neg sentiment scores for each airline
twitter.df = ddply(all.scores, c('company_name'), summarise, pos.count = sum (very.pos), neg.count = sum(very.neg))
#and here the general sentiment
twitter.df$all.count = twitter.df$pos.count + twitter.df$neg.count
#now in order to be able to compare data sets we normalise the sentiment score to be a percentage
twitter.df$score = round (100 * twitter.df$pos.count / twitter.df$all.count)
#and to help understand our data, order by our now normalised score
orderBy(~-score, twitter.df)
#now let's get data source no. 2, for this we need some extra packages to help us parse HTML
write.csv(twitter.df, file="SentimentalStartup.csv")
#We can view any of these data frames using the View function, e.g.: View(delta.scores)
View(delta.scores)
#we combine all data frames into 1
all.scores = rbind(CredR.score,quikr.score,Naaptol.score,Pepperfry.score,Faasos.score)
all.scores
#skim only the most positive or negative tweets to throw away noise near 0
all.scores$very.pos = as.numeric( all.scores$score >= 2)
all.scores$very.neg = as.numeric( all.scores$score <= -2)
#now we construct the twitter data frame and simultaneously compute the pos/neg sentiment scores for each airline
twitter.df = ddply(all.scores, c('companyName'), summarise, pos.count = sum (very.pos), neg.count = sum(very.neg))
#and here the general sentiment
twitter.df$all.count = twitter.df$pos.count + twitter.df$neg.count
#now in order to be able to compare data sets we normalise the sentiment score to be a percentage
twitter.df$score = round (100 * twitter.df$pos.count / twitter.df$all.count)
#and to help understand our data, order by our now normalised score
orderBy(~-score, twitter.df)
#now let's get data source no. 2, for this we need some extra packages to help us parse HTML
write.csv(twitter.df, file="SentimentalStartup.csv")
#here we invoke the function above for each of our airlines
CredR.score = collect.and.score("#olx","CredR", pos.words, neg.words)
quikr.score = collect.and.score("#quikrcars","Quikr", pos.words, neg.words)
Naaptol.score = collect.and.score("#Naaptol","Maaptol", pos.words, neg.words)
Pepperfry.score = collect.and.score("#Pepperfry","Pepperfry", pos.words, neg.words)
Faasos.score = collect.and.score("#flipkart","Faasos", pos.words, neg.words)
#We can view any of these data frames using the View function, e.g.: View(delta.scores)
View(delta.scores)
#we combine all data frames into 1
all.scores = rbind(CredR.score,quikr.score,Naaptol.score,Pepperfry.score,Faasos.score)
all.scores
#skim only the most positive or negative tweets to throw away noise near 0
all.scores$very.pos = as.numeric( all.scores$score >= 2)
all.scores$very.neg = as.numeric( all.scores$score <= -2)
#now we construct the twitter data frame and simultaneously compute the pos/neg sentiment scores for each airline
twitter.df = ddply(all.scores, c('companyName'), summarise, pos.count = sum (very.pos), neg.count = sum(very.neg))
#and here the general sentiment
twitter.df$all.count = twitter.df$pos.count + twitter.df$neg.count
#now in order to be able to compare data sets we normalise the sentiment score to be a percentage
twitter.df$score = round (100 * twitter.df$pos.count / twitter.df$all.count)
#and to help understand our data, order by our now normalised score
orderBy(~-score, twitter.df)
#now let's get data source no. 2, for this we need some extra packages to help us parse HTML
write.csv(twitter.df, file="SentimentalStartup.csv")
#here we invoke the function above for each of our airlines
CredR.score = collect.and.score("#olx","CredR", pos.words, neg.words)
quikr.score = collect.and.score("#quikrcars","Quikr", pos.words, neg.words)
Naaptol.score = collect.and.score("#ola","Naaptol", pos.words, neg.words)
Pepperfry.score = collect.and.score("#Pepperfry","Pepperfry", pos.words, neg.words)
Faasos.score = collect.and.score("#flipkart","Faasos", pos.words, neg.words)
#We can view any of these data frames using the View function, e.g.: View(delta.scores)
View(delta.scores)
#we combine all data frames into 1
all.scores = rbind(CredR.score,quikr.score,Naaptol.score,Pepperfry.score,Faasos.score)
all.scores
#skim only the most positive or negative tweets to throw away noise near 0
all.scores$very.pos = as.numeric( all.scores$score >= 2)
all.scores$very.neg = as.numeric( all.scores$score <= -2)
#now we construct the twitter data frame and simultaneously compute the pos/neg sentiment scores for each airline
twitter.df = ddply(all.scores, c('companyName'), summarise, pos.count = sum (very.pos), neg.count = sum(very.neg))
#and here the general sentiment
twitter.df$all.count = twitter.df$pos.count + twitter.df$neg.count
#now in order to be able to compare data sets we normalise the sentiment score to be a percentage
twitter.df$score = round (100 * twitter.df$pos.count / twitter.df$all.count)
#and to help understand our data, order by our now normalised score
orderBy(~-score, twitter.df)
#now let's get data source no. 2, for this we need some extra packages to help us parse HTML
write.csv(twitter.df, file="SentimentalStartup.csv")
#install.packages(c("rjson", "bit64", "httr", "doBy", "XML", "base64enc"))
library(devtools)
#install_github("geoffjentry/twitteR")
#install_github('R-package','quandl')
library(plyr)
library(httr)
library(doBy)
library(Quandl)
library(twitteR)
api_key = "PA1FaZycV1Op5FGJdKIPE1coa"
api_secret = "jiUQYtBD9OtvVVygDCXDGn4xI8pBj3xdLKRyOz4wOEIoFbsgE4"
access_token = "2261152723-Va9KBatH7hU0UmlaTQSudQdBNl1omIfnU0orb8J"
access_token_secret = "vadLvMXSxgdRCSUIGKFcg3dJgJjwn60V4BrlnEzRA3G30"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
hu.liu.pos = scan('positive-words.txt', what='character', comment.char=';')
hu.liu.neg = scan('negative-words.txt', what='character', comment.char=';')
#now we can add some domain-specific terminolgy
pos.words = c(hu.liu.pos, 'Indianstartup','good','Startupbusiness')
neg.words = c(hu.liu.neg, 'loss', 'notgood', 'badidea','Startuploss')
#our first function
score.sentence <- function(sentence, pos.words, neg.words) {
#here some basic cleaning
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl::]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
sentence = tolower(sentence)
#basic data structure construction
word.list = str_split(sentence, '\\s+')
words = unlist(word.list)
#here we count the number of words that are positive and negative
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
#throw away those that didn't match
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
#compute the sentiment score
score = sum(pos.matches) - sum(neg.matches)
return(score)
}
#our second function that takes an array of sentences and sentiment analyses them
score.sentiment <- function(sentences, pos.words, neg.words) {
require(plyr)
require(stringr)
#here any sentence/tweet that causes an error is given a sentiment score of 0 (neutral)
scores = laply(sentences, function(sentence, pos.words, neg.words) {
tryCatch(score.sentence(sentence, pos.words, neg.words ), error=function(e) 0)
}, pos.words, neg.words)
#now we construct a data frame
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
#our third function, that communicates with twitter and then scores each of the tweets returned
collect.and.score <- function (handle, companyName, pos.words, neg.words) {
tweets = searchTwitter(handle, n=500)
text = laply(tweets, function(t) t$getText())
score = score.sentiment(text, pos.words, neg.words)
score$companyName = companyName
#  score$code = code
return (score)
}
#here we invoke the function above for each of our airlines
CredR.score = collect.and.score("#olx","CredR", pos.words, neg.words)
quikr.score = collect.and.score("#quikrcars","Quikr", pos.words, neg.words)
Naaptol.score = collect.and.score("#ola","Naaptol", pos.words, neg.words)
Pepperfry.score = collect.and.score("#Pepperfry","Pepperfry", pos.words, neg.words)
Faasos.score = collect.and.score("#flipkart","Faasos", pos.words, neg.words)
#We can view any of these data frames using the View function, e.g.: View(delta.scores)
View(delta.scores)
#we combine all data frames into 1
all.scores = rbind(CredR.score,quikr.score,Naaptol.score,Pepperfry.score,Faasos.score)
all.scores
#skim only the most positive or negative tweets to throw away noise near 0
all.scores$very.pos = as.numeric( all.scores$score >= 2)
all.scores$very.neg = as.numeric( all.scores$score <= -2)
#now we construct the twitter data frame and simultaneously compute the pos/neg sentiment scores for each airline
twitter.df = ddply(all.scores, c('companyName'), summarise, pos.count = sum (very.pos), neg.count = sum(very.neg))
#and here the general sentiment
twitter.df$all.count = twitter.df$pos.count + twitter.df$neg.count
#now in order to be able to compare data sets we normalise the sentiment score to be a percentage
twitter.df$score = round (100 * twitter.df$pos.count / twitter.df$all.count)
#and to help understand our data, order by our now normalised score
orderBy(~-score, twitter.df)
#now let's get data source no. 2, for this we need some extra packages to help us parse HTML
write.csv(twitter.df, file="StartupSenti.csv")
installedPackages <- .packages(all.available = TRUE)
availablePackages <- available.packages()[,1]
install.packages(availablePackages)
install.packages(availablePackages)
install.packages(availablePackages)
#reading the dataset
mydataset <- read.csv("Sampleadm.csv",stringsAsFactors = T)
str(my_dataset)
#below labes are given in the meta data
my_dataset$Education <- factor(my_dataset$Education, levels = c(1,2,3,4,5), labels=c("BC", "C", "UG", "MSc", "PhD"))
my_dataset$EnvironmentSatisfaction <- factor(my_dataset$EnvironmentSatisfaction, levels = c(1:4),labels=c("Low", "Medium", "High", "v. High"))
my_dataset$JobInvolvement <- factor(my_dataset$JobInvolvement, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$JobLevel <- factor(my_dataset$JobLevel) #insufficient information to do more
my_dataset$JobSatisfaction <- factor(my_dataset$JobSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating, levels = c(1:4), labels=c("Low", "Good", "Excellent", "Outstanding"))
my_dataset$RelationshipSatisfaction <- factor(my_dataset$RelationshipSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$StockOptionLevel <- factor(my_dataset$StockOptionLevel) #don't have more information
my_dataset$WorkLifeBalance <- factor(my_dataset$WorkLifeBalance, levels = c(1:4), labels=c("Bad", "Good", "Better", "Best"))
summary(my_dataset)
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating)
table(my_dataset$JobRole)
table(my_dataset$MaritalStatus)
#Few variables are pointless, So lets get rid of them
my_dataset <- my_dataset[,-9]
my_dataset <- my_dataset[, -19]
#finding the missing value
sapply(my_dataset,function(x) sum(is.na(x)))
set.seed(1337)
#checking for outliers
boxplot(subset(my_dataset, select=c(1,6,18,20,21:24))) #nothing too obvious here
boxplot(subset(my_dataset, select=c(26:29))) #May be something in years at company
boxplot(my_dataset$DailyRate)
boxplot(my_dataset$HourlyRate)
boxplot(my_dataset$MonthlyIncome) #possibly some here too
boxplot(my_dataset$MonthlyRate)
table(my_dataset$Attrition)
#i prefer the attrition factor as 0/1 for convenience
my_dataset$Attrition <- factor(my_dataset$Attrition, labels=c(0,1), levels=c("No", "Yes"))
table(my_dataset$Attrition)
mydataset <- read.csv("Sampleadm.csv",stringsAsFactors = T)
str(my_dataset)
mydataset <- read.csv("Sampleadm.csv",stringsAsFactors = T)
mydataset <- read.csv("Sampleadm.csv",stringsAsFactors = T)
mydataset <- read.csv("Sampleadm.csv",stringsAsFactors = T)
mydataset <- read.csv("Sampleadm.csv",stringsAsFactors = T)
mydataset <- read.csv("Sampleadm.csv",stringsAsFactors = T)
#reading the dataset
mydataset <- read.csv("Sampleadm.csv",stringsAsFactors = T)
my_dataset <- read.csv("Sampleadm.csv", stringsAsFactors = T)
setwd("C:/Nithi/Github/MyProgramming")
my_dataset <- read.csv("Sampleadm.csv", stringsAsFactors = T)
str(my_dataset)
#let's deal with these other factors: F1
my_dataset$Education <- factor(my_dataset$Education, levels = c(1,2,3,4,5), labels=c("BC", "C", "UG", "MSc", "PhD"))
my_dataset$EnvironmentSatisfaction <- factor(my_dataset$EnvironmentSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$JobInvolvement <- factor(my_dataset$JobInvolvement, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$JobLevel <- factor(my_dataset$JobLevel) #insufficient information to do more
my_dataset$JobSatisfaction <- factor(my_dataset$JobSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating, levels = c(1:4), labels=c("Low", "Good", "Excellent", "Outstanding"))
my_dataset$RelationshipSatisfaction <- factor(my_dataset$RelationshipSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v. High"))
my_dataset$StockOptionLevel <- factor(my_dataset$StockOptionLevel) #don't have more information
my_dataset$WorkLifeBalance <- factor(my_dataset$WorkLifeBalance, levels = c(1:4), labels=c("Bad", "Good", "Better", "Best"))
summary(my_dataset)
#I'm missing levels 0 and 1 in PerformanceRating, but the rest of my categoricals seem fine
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating)
#however, as JobRole displays other, let's check more precisely
table(my_dataset$JobRole)
#a couple of the variables are pointless, so let's get rid of them
my_dataset <- my_dataset[,-9] #EmployeeCount -- always 1
my_dataset <- my_dataset[, -19] #over 18 -- always Y; strictly speaking we should confirm this, but under time pressure it's safe to assume this varibale isn't needed
#mising values:
sapply(my_dataset,function(x) sum(is.na(x)))
#outliers
boxplot(subset(my_dataset, select=c(1,6,18,20,21:24))) #nothing too obvious here
boxplot(subset(my_dataset, select=c(26:29))) #May be something in years at company
boxplot(my_dataset$DailyRate)
boxplot(my_dataset$HourlyRate)
boxplot(my_dataset$MonthlyIncome) #possibly some here too
boxplot(my_dataset$MonthlyRate)
#outliers
boxplot(subset(my_dataset, select=c(1,6,18,20,21:24))) #nothing too obvious here
boxplot(my_dataset$MonthlyRate)
sapply(my_dataset,function(x) sum(is.na(x)))
boxplot(my_dataset$MonthlyRate)
boxplot(my_dataset$HourlyRate)
boxplot(my_dataset$MonthlyIncome) #possibly some here too
boxplot(subset(my_dataset, select=c(26:29))) #May be something in years at company
boxplot(my_dataset$MonthlyIncome) #possibly some here too
my_dataset$Attrition <- factor(my_dataset$Attrition, labels=c(0,1), levels=c("No", "Yes"))
table(my_dataset$Attrition)
